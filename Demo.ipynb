{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook aims to use transfer learing to working as image matching tools for fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "There are two groups of photos, one group(targets) repersent the different products avable in, another group(querires) is actual photos taken by employees and user. The tasks is to pair the queriry photo to corresponds photo in target group.\n",
    "\n",
    "In the seeting of this task,one querirring seesion may contain few hundredrs querires and few hundredrs targets. But the targets may change over each session.\n",
    "\n",
    "(see the moviation part belowe for such wired setting for the task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Idea\n",
    "\n",
    "This image pairing probelm can be restated as finding the most similar photo of query in targets set.\\\n",
    "The photo is first embebed into a vector space, then simirity can then be defined as the angle between the two vectors.\\\n",
    "The inititial trial is to use a pretrained featurizer to embed the image, then identify the target photo with max simirity.\n",
    "\n",
    "(note: more potential varyation exist, see discussion for detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet:\t accuracy = 66.67 %\n",
      "DenseNet:\t accuracy = 73.33 %\n",
      "ConvNeXt:\t accuracy = 46.67 %\n",
      "EfficientNet:\t accuracy = 80.00 %\n",
      "GoogLeNet:\t accuracy = 60.00 %\n",
      "Inception3:\t accuracy = 66.67 %\n",
      "MaxVit:\t accuracy = 66.67 %\n",
      "MNASNet:\t accuracy = 66.67 %\n",
      "MobileNetV3:\t accuracy = 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# helper code and importssssssss\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "#main test code\n",
    "def main():\n",
    "    data_path = Path(\"./UNIQOExample\")\n",
    "    show_architecture = False\n",
    "    show_result_by_graph = False\n",
    "    result = []\n",
    "    transforms_default = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(224, antialias=True),\n",
    "    ])\n",
    "    transforms_maxvit_t = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(224, antialias=True),\n",
    "    ])\n",
    "    models_to_try = [\n",
    "        (models.alexnet, models.AlexNet_Weights.DEFAULT, transforms_default), \n",
    "        (models.densenet121, models.DenseNet121_Weights.DEFAULT, transforms_default),\n",
    "        (models.convnext_tiny, models.ConvNeXt_Tiny_Weights.DEFAULT, transforms_default), \n",
    "        (models.efficientnet_b0, models.EfficientNet_B0_Weights.DEFAULT, transforms_default), \n",
    "        (models.googlenet, models.GoogLeNet_Weights.DEFAULT, transforms_default),\n",
    "        (models.inception_v3, models.Inception_V3_Weights.DEFAULT, transforms_default),\n",
    "        (models.maxvit_t, models.MaxVit_T_Weights.DEFAULT, transforms_maxvit_t), #used 224 as input size\n",
    "        (models.mnasnet0_5, models.MNASNet0_5_Weights.DEFAULT, transforms_default),\n",
    "        (models.mobilenet_v3_small, models.MobileNet_V3_Small_Weights.DEFAULT, transforms_default),\n",
    "    ] #tuple(model_constructor, pretrain_wieghts, transforms for inputs image)\n",
    "\n",
    "    for model_constructor, wieghts, transforms_needed in models_to_try:\n",
    "        \n",
    "        # make model by architecture specified and turn off the classfier in the orginal model\n",
    "        model = make_features_extractor(model_constructor, wieghts)\n",
    "        if show_architecture: \n",
    "            print(model)\n",
    "        \n",
    "        (inputs, input_files, labels), (match_targets, match_files, match_labels) = load_data(data_path, transforms_needed)\n",
    "        \n",
    "        # featurize input and match samples, then normalize to 1(by row aka each sample)\n",
    "        outputs = featurize(model, inputs)\n",
    "        match_sapce = featurize(model, match_targets)\n",
    "\n",
    "        # compute cos_sim for all input with and match target\n",
    "        # shape of torch.mm = (# of inputs, # of features) * (# of features, # of match samples) \n",
    "        # shape of torch.mm = (# of inputs, # of match samples)\n",
    "        match_sapce = torch.transpose(match_sapce, dim0=0, dim1=1)\n",
    "        cos_similaritys, top_classes = torch.mm(outputs, match_sapce).topk(1)\n",
    "        \n",
    "        # match the predecition to file name of match samples\n",
    "        top_classes = torch.tensor( list(match_labels[i.item()] for i in top_classes))\n",
    "        \n",
    "        accuracy = labels == top_classes\n",
    "        accuracy = (accuracy.sum()/accuracy.shape[0]).item()\n",
    "        result.append((model._get_name(), accuracy))\n",
    "\n",
    "        print(f\"{model.__class__.__name__}:\\t accuracy = {accuracy*100:.2f} %\")\n",
    "        if show_result_by_graph: \n",
    "            show_helper(input_files, labels, top_classes)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def make_features_extractor(model_constructor, wieghts):\n",
    "    \"\"\"constures a model\n",
    "\n",
    "    Args:\n",
    "        model_constructor (_type_): _description_\n",
    "        wieghts (_type_): _description_\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: _description_\n",
    "        NotImplementedError: _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    if wieghts is None:\n",
    "        model = model_constructor(pretrained = True)\n",
    "    model = model_constructor(weights = wieghts)\n",
    "    model.eval()\n",
    "    \n",
    "    if hasattr(model, \"classifier\") and not hasattr(model, \"fc\"):\n",
    "        model.classifier = Identity()\n",
    "    elif not hasattr(model, \"classifier\") and hasattr(model, \"fc\"):\n",
    "        model.fc = Identity()\n",
    "    elif not hasattr(model, \"classifier\") and not hasattr(model, \"fc\"):\n",
    "        raise NotImplementedError(f\"{model.__class__.__name__} have no features layer nor fc layer\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"ERROR, {model.__class__.__name__} have both????????\")\n",
    "    \n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_data(data_path, transforms_needed = None):\n",
    "    \"\"\"load data deom datapath and transform\n",
    "\n",
    "    Args:\n",
    "        (data_path: Path): path object\n",
    "        (transforms_needed:torchvision.transforms): transforms the import files\n",
    "\n",
    "    Returns:\n",
    "        (inputs: tensor(B, C, H, W), input_files: list[Path], labels: list[int]), (match_targets: tensor(B, C, H, W), match_labels: list[int]): as shown\n",
    "    \"\"\"\n",
    "    \n",
    "    if transforms_needed is None:\n",
    "        default_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(224, antialias=True),\n",
    "        ]) \n",
    "        transforms_needed = default_transforms\n",
    "    \n",
    "    inputs_path = data_path/\"queries\"\n",
    "    inputs, labels, input_files = [], [], []\n",
    "    for file in inputs_path.glob(\"*/*\"):\n",
    "        input_files.append(file)\n",
    "        labels.append(int(file.parts[-2]))\n",
    "        inputs.append(transforms_needed(Image.open(file)))\n",
    "    inputs = torch.stack(inputs)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    match_target_path = data_path/\"targets\"\n",
    "    match_targets, match_files, match_labels= [], [], []\n",
    "    for file in match_target_path.glob(\"*\"):\n",
    "        match_files.append(file)\n",
    "        match_labels.append(int(file.stem))\n",
    "        match_targets.append(transforms_needed(Image.open(file)))\n",
    "    match_targets = torch.stack(match_targets)\n",
    "    match_labels = torch.tensor(match_labels)\n",
    "\n",
    "    return (inputs, input_files, labels), (match_targets, match_files, match_labels)\n",
    "\n",
    "def featurize(model: torch.nn.Module, inputs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"take inputs tensor and do forward() to featurize the inputs image tensor, \n",
    "    as different model have diffent output shape after featurise, this function also adjust the shpae\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): model\n",
    "        inputs (torch.Tensor): (B, C, H, W) C ==3\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: Unexcepted shape of output\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: _description_\n",
    "    \"\"\"\n",
    "    outputs = model(inputs)\n",
    "    if outputs.dim() == 4:\n",
    "        outputs = torch.nn.functional.adaptive_avg_pool2d(outputs, output_size=(1,1))\n",
    "        outputs = torch.squeeze(outputs, dim = tuple(range(2, outputs.dim())))\n",
    "    elif outputs.dim() == 2:\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unexcepted deminsion shape {outputs.shape} from {model.__class__.__name__}\")\n",
    "    outputs = nn.functional.normalize(outputs, p=2, dim=1)\n",
    "    return outputs\n",
    "\n",
    "def show_helper(input_files, labels, top_classes):\n",
    "    \"\"\"some helper code for visualztion\n",
    "\n",
    "    Args:\n",
    "        input_files (_type_): _description_\n",
    "        labels (_type_): _description_\n",
    "        top_classes (_type_): _description_\n",
    "    \"\"\"\n",
    "    for file, label, predecition in zip(input_files, labels, top_classes):\n",
    "        \n",
    "        label, predecition = label.item(), predecition.item()\n",
    "\n",
    "        predecition = Image.open(file.parents[2]/\"targets\"/(str(predecition)+\".jpg\"))\n",
    "        file = Image.open(file)\n",
    "        # label = Image.open(Path(data_path)/\"target\"/f\"{label}.jpg\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,7))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(file)\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(predecition)\n",
    "\n",
    "#driver\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work on\n",
    "1. if labeled data is avalable, try SIAMESE CNN\n",
    "2. Use other kinds of meteric to determin the simirity, i.e. L2-norm, L1-norm for non-unity vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Motivation\n",
    "In a road trip with my friends, one of my friends who own a fashion bussess needed to work in hotel room, while watching he works, I found that most of his work are pretty mechanical. There are two major taks:\n",
    "1. Product matching, match the product photo taken by employee to sample photo from supplier for interal invetory system.\n",
    "2. Table recognition, manul input sale data from photo\n",
    "\n",
    "\"it should be a easy task for AI\" I thougth it should be easy to bulid a tool with existig resourse. \\\n",
    "I was worng. VERY WRONG\\\n",
    "Atleast not easy for me who know nothing abour programming at that time.\\\n",
    "\n",
    "The table recognition task, i tried few online tools and most of them are horrible at idenfity hand-written table. Then I switched to use paddlepaddle(a ML package by China internet giant  BAIDU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
